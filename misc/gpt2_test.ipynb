{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ranking-demonstration",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AutoTokenizer, AutoModelForCausalLM, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77753607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_and_tokenizer(model_class, model_name):\n",
    "  CACHE_DIR = '/scratch/gpfs/kw1166/.cache/'\n",
    "  lm_model = model_class.from_pretrained(\n",
    "      model_name,\n",
    "      output_hidden_states=True,\n",
    "      local_files_only=True,\n",
    "      cache_dir=CACHE_DIR\n",
    "  )\n",
    "\n",
    "  tokenizer = AutoTokenizer.from_pretrained(\n",
    "      model_name,\n",
    "      add_predix_space=True,\n",
    "      local_files_only=True,\n",
    "      cache_dir=CACHE_DIR,\n",
    "      use_fast=False\n",
    "  )\n",
    "\n",
    "  return (lm_model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "spatial-syndication",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# CACHE_DIR = '/scratch/gpfs/kw1166/.cache/'\n",
    "# tokenizer_2 = GPT2Tokenizer.from_pretrained('gpt2-xl',\n",
    "#                                           add_prefix_space=True,\n",
    "#                                           cache_dir=CACHE_DIR)\n",
    "# tokenizer_2.pad_token = tokenizer_2.eos_token\n",
    "\n",
    "# lm_model_2 = GPT2LMHeadModel.from_pretrained(\"gpt2-xl\",\n",
    "#                                            output_hidden_states=True,\n",
    "#                                            cache_dir=CACHE_DIR)\n",
    "\n",
    "# lm_model, tokenizer = get_model_and_tokenizer(AutoModelForCausalLM, 'gpt2-xl')\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "lm_model, tokenizer = get_model_and_tokenizer(AutoModelForMaskedLM, 'bert-base-cased')\n",
    "lm_model2, tokenizer2 = get_model_and_tokenizer(AutoModelForMaskedLM, 'bert-large-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "intense-showcase",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=28996, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-duration",
   "metadata": {},
   "source": [
    "### All sentences Batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0020d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UNK] [CLS] [SEP] [MASK]\n",
      "PreTrainedTokenizer(name_or_path='bert-base-cased', vocab_size=28996, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}) PreTrainedTokenizer(name_or_path='bert-large-uncased', vocab_size=30522, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode([100,101,102,103]))\n",
    "print(tokenizer, tokenizer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "surrounded-liver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,   103,   102,     0,     0,     0,     0],\n",
      "        [  101, 19082,   103,   102,     0,     0,     0],\n",
      "        [  101, 19082,  1362,   103,   102,     0,     0],\n",
      "        [  101, 19082,  1362,  1175,   103,   102,     0],\n",
      "        [  101, 19082,  1362,  1175,  1128,   103,   102],\n",
      "        [  101,  1362,  1175,  1128,  1132,   103,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "sentences = ['[MASK]',\n",
    " 'hello [MASK]',\n",
    " 'hello world [MASK]',\n",
    " 'hello world there [MASK]',\n",
    " 'hello world there you [MASK]',\n",
    " 'world there you are [MASK]'\n",
    " ]\n",
    "# input_ids = tokenizer(sentences, padding=True, return_tensors='pt')\n",
    "input_ids = tokenizer.batch_encode_plus(sentences, padding=True, return_tensors='pt')\n",
    "# input_ids2 = tokenizer2.batch_encode_plus(sentences, padding=True, return_tensors='pt')\n",
    "\n",
    "print(input_ids)\n",
    "# print(input_ids2)\n",
    "lm_outputs = lm_model(**input_ids)\n",
    "transformer_hidden_states = lm_outputs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "combined-integral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([101, 103, 102,   0,   0,   0,   0]) tensor([1, 1, 1, 0, 0, 0, 0])\n",
      "tensor([  101, 19082,   103,   102,     0,     0,     0]) tensor([1, 1, 1, 1, 0, 0, 0])\n",
      "tensor([  101, 19082,  1362,   103,   102,     0,     0]) tensor([1, 1, 1, 1, 1, 0, 0])\n",
      "tensor([  101, 19082,  1362,  1175,   103,   102,     0]) tensor([1, 1, 1, 1, 1, 1, 0])\n",
      "tensor([  101, 19082,  1362,  1175,  1128,   103,   102]) tensor([1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([ 101, 1362, 1175, 1128, 1132,  103,  102]) tensor([1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "for a, b in zip(input_ids['input_ids'], input_ids['attention_mask']):\n",
    "    print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a623412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1323,  0.3435,  0.1365,  ...,  0.2833, -0.2909,  0.3727],\n",
      "         [ 0.4390, -0.0905,  0.2218,  ..., -0.3375,  0.3586,  0.3006],\n",
      "         [ 1.1752,  0.4498, -0.6960,  ...,  0.1995,  0.8212, -0.1582],\n",
      "         ...,\n",
      "         [ 0.0307, -0.1238,  0.1624,  ..., -0.3096,  0.3233,  0.1251],\n",
      "         [ 0.2535,  0.1223, -0.2642,  ..., -0.1411,  0.5176,  0.5266],\n",
      "         [ 0.1693,  0.1635, -0.2344,  ..., -0.1314,  0.5437,  0.3968]],\n",
      "\n",
      "        [[ 0.4933,  0.3707,  0.2677,  ..., -0.2835,  0.5303, -0.0569],\n",
      "         [ 0.5455, -0.4865,  0.7949,  ..., -0.5479,  0.1502,  0.0675],\n",
      "         [ 0.4508,  0.3423,  0.4286,  ..., -0.5160,  0.4282, -0.0413],\n",
      "         ...,\n",
      "         [ 0.1143,  0.0660,  0.3881,  ..., -0.2771,  0.4815,  0.0912],\n",
      "         [ 0.1674,  0.0898,  0.4107,  ..., -0.3823,  0.4465,  0.0524],\n",
      "         [ 0.3055,  0.4224,  0.3344,  ..., -0.3969,  0.4021, -0.0621]],\n",
      "\n",
      "        [[ 0.3388,  0.4806,  0.3763,  ..., -0.2291,  0.5335, -0.1023],\n",
      "         [ 0.5272, -0.5086,  0.7069,  ..., -0.4582,  0.3148,  0.0279],\n",
      "         [ 0.4278,  0.3967,  0.3871,  ..., -0.5092, -0.1514, -0.2173],\n",
      "         ...,\n",
      "         [ 0.6080,  0.4527,  0.0436,  ..., -0.1208,  1.1093, -0.4716],\n",
      "         [ 0.1259,  0.1776,  0.5673,  ..., -0.0498,  0.6351,  0.0410],\n",
      "         [ 0.1718,  0.1232,  0.4832,  ..., -0.2371,  0.5134,  0.1391]],\n",
      "\n",
      "        [[ 0.3615,  0.3500,  0.3424,  ..., -0.2873,  0.4625, -0.1784],\n",
      "         [ 0.5318, -0.6173,  0.6400,  ..., -0.3963,  0.2671, -0.0152],\n",
      "         [ 0.4932,  0.1638,  0.5611,  ..., -0.3671, -0.0975, -0.2913],\n",
      "         ...,\n",
      "         [ 0.2733,  0.1102,  0.6711,  ..., -0.2990,  0.3508, -0.1389],\n",
      "         [ 0.8346,  0.1917,  0.0622,  ..., -0.1220,  1.2775, -0.4157],\n",
      "         [ 0.1763,  0.1010,  0.5218,  ..., -0.0821,  0.5183,  0.0852]],\n",
      "\n",
      "        [[ 0.3330,  0.2930,  0.2944,  ..., -0.2985,  0.4290, -0.0687],\n",
      "         [ 0.5024, -0.6006,  0.6971,  ..., -0.4685,  0.4298,  0.0079],\n",
      "         [ 0.6047,  0.1221,  0.4204,  ..., -0.5348,  0.3039, -0.4099],\n",
      "         ...,\n",
      "         [ 0.9636,  0.0520,  0.3796,  ..., -0.3316,  0.2686,  0.3360],\n",
      "         [ 0.4774,  0.1868,  0.6205,  ..., -0.2933,  0.1455, -0.1537],\n",
      "         [ 0.4781,  0.1739, -0.0270,  ..., -0.0408,  1.2526, -0.1772]],\n",
      "\n",
      "        [[ 0.3338,  0.1604,  0.2617,  ..., -0.3070,  0.4054, -0.1643],\n",
      "         [ 0.6409, -0.2253,  0.5385,  ..., -0.3512,  0.3348, -0.3762],\n",
      "         [ 0.7685, -0.2549, -0.1876,  ...,  0.0467,  0.3778,  0.0230],\n",
      "         ...,\n",
      "         [ 0.0572, -0.0173, -0.0091,  ..., -0.3100, -0.0422,  0.0176],\n",
      "         [ 0.1791,  0.0635,  0.2580,  ..., -0.4397,  0.4042, -0.2846],\n",
      "         [ 1.1464,  0.4223,  0.2713,  ..., -1.0638,  0.7719, -0.4194]]],\n",
      "       grad_fn=<NativeLayerNormBackward>)\n"
     ]
    }
   ],
   "source": [
    "embeddings = transformer_hidden_states[-1]\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "czech-malpractice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]],\n",
       "\n",
       "        [[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_mask = input_ids['attention_mask']\n",
    "attn_mask = attn_mask.unsqueeze(-1).expand(embeddings.shape)\n",
    "attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "statutory-touch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 7, 768]), torch.Size([6, 7]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape, input_ids['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574dcd56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "constant-anaheim",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0588,  0.1301,  0.2188,  ..., -4.4375,  0.7642,  0.2327],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000]],\n",
       "\n",
       "        [[ 1.0588,  0.1301,  0.2188,  ..., -4.4375,  0.7642,  0.2327],\n",
       "         [-0.2217,  0.3650,  0.6309,  ..., -1.3968,  0.1588,  0.1691],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000]],\n",
       "\n",
       "        [[ 1.0588,  0.1301,  0.2188,  ..., -4.4375,  0.7642,  0.2327],\n",
       "         [-0.2217,  0.3650,  0.6309,  ..., -1.3968,  0.1588,  0.1691],\n",
       "         [ 0.0775,  0.1162,  0.7933,  ..., -1.5378, -0.0938,  0.9396],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.0588,  0.1301,  0.2188,  ..., -4.4375,  0.7642,  0.2327],\n",
       "         [-0.2217,  0.3650,  0.6309,  ..., -1.3968,  0.1588,  0.1691],\n",
       "         [ 0.4922,  0.5799,  0.3748,  ..., -1.2212,  0.4659, -0.0181],\n",
       "         [ 1.1679, -0.2796, -0.2864,  ..., -1.5432,  0.7543, -0.8277],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000]],\n",
       "\n",
       "        [[ 1.0588,  0.1301,  0.2188,  ..., -4.4375,  0.7642,  0.2327],\n",
       "         [-0.2217,  0.3650,  0.6309,  ..., -1.3968,  0.1588,  0.1691],\n",
       "         [ 0.4922,  0.5799,  0.3748,  ..., -1.2212,  0.4659, -0.0181],\n",
       "         [ 1.1679, -0.2796, -0.2864,  ..., -1.5432,  0.7543, -0.8277],\n",
       "         [ 0.2596,  0.3958,  0.6677,  ..., -1.7543,  0.9028,  0.6392]],\n",
       "\n",
       "        [[-0.4205,  0.6139,  1.0727,  ..., -4.7529,  1.2246, -0.4381],\n",
       "         [-0.1484, -0.2284,  0.3114,  ..., -1.4831,  0.4668, -0.4868],\n",
       "         [-0.0156, -0.6524,  0.0106,  ..., -1.2291,  0.0511, -0.2832],\n",
       "         [-1.7830,  0.0059,  0.2923,  ..., -1.4245, -0.0341,  0.6478],\n",
       "         [-0.9276, -0.2381, -0.5200,  ..., -0.9828,  0.4576,  0.4824]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings * attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "allied-rating",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1.0588,   0.1301,   0.2188,  ...,  -4.4375,   0.7642,   0.2327],\n",
       "        [  0.8371,   0.4952,   0.8497,  ...,  -5.8343,   0.9230,   0.4018],\n",
       "        [  0.9147,   0.6114,   1.6430,  ...,  -7.3722,   0.8292,   1.3414],\n",
       "        ...,\n",
       "        [  2.4972,   0.7955,   0.9381,  ...,  -8.5987,   2.1432,  -0.4440],\n",
       "        [  2.7568,   1.1913,   1.6058,  ..., -10.3531,   3.0460,   0.1953],\n",
       "        [ -3.2951,  -0.4990,   1.1670,  ...,  -9.8724,   2.1661,  -0.0779]],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.sum(embeddings * attn_mask, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "recent-enforcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [('Hello'),\n",
    "             ('Hello', 'world'),\n",
    "             ('Hello', 'world', 'hello'),\n",
    "             ('Hello', 'world', 'there'),\n",
    "             ('Hello', 'world', 'there', 'you'),\n",
    "             ('Hello', 'world', 'there', 'you', 'are'),\n",
    "            ('world', 'there', 'you', 'are', 'flying')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "naked-separation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[18435, 50256, 50256, 50256, 50256],\n",
       "        [18435,   995, 50256, 50256, 50256],\n",
       "        [18435,   995, 23748, 50256, 50256],\n",
       "        [18435,   995,   612, 50256, 50256],\n",
       "        [18435,   995,   612,   345, 50256],\n",
       "        [18435,   995,   612,   345,   389],\n",
       "        [  995,   612,   345,   389,  7348]]), 'attention_mask': tensor([[1, 0, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer(sentences, padding=False) # doesn't work\n",
    "input_dict = tokenizer.batch_encode_plus(sentences, is_split_into_words=True, padding=True, return_tensors='pt')\n",
    "input_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "brief-onion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "myvec =  transformer_hidden_states[-1]\n",
    "myvec2 = myvec/torch.norm(myvec, p=2)\n",
    "myvec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "legendary-profit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tok_to_str = tokenizer.batch_decode(input_ids['input_ids'],\n",
    "#                                     skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-elephant",
   "metadata": {},
   "source": [
    "### One sentence at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 768])\n",
      "tensor([[[ 0.4933,  0.3707,  0.2677,  ..., -0.2835,  0.5303, -0.0569],\n",
      "         [ 0.5455, -0.4865,  0.7949,  ..., -0.5479,  0.1502,  0.0675],\n",
      "         [ 0.4508,  0.3423,  0.4286,  ..., -0.5160,  0.4282, -0.0413],\n",
      "         [ 0.9906,  0.0174, -0.2410,  ..., -0.1005,  1.2535, -0.3444]]],\n",
      "       grad_fn=<NativeLayerNormBackward>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "windows = [(101,19082,103,102)]\n",
    "input_ids = torch.tensor(windows)\n",
    "\n",
    "data_dl = data.DataLoader(input_ids, batch_size=1, shuffle=False)\n",
    "for batch_idx, batch in enumerate(data_dl):\n",
    "    if batch_idx == 0:\n",
    "        # batch = batch.to(\"cuda\")\n",
    "        model_output = lm_model(batch)\n",
    "transformer_hidden_states = model_output[-1]\n",
    "print(transformer_hidden_states[-1].shape)\n",
    "print(transformer_hidden_states[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "palestinian-petersburg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[101, 103, 102]]), 'token_type_ids': tensor([[0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1]])}\n",
      "torch.Size([1, 3, 768])\n",
      "tensor([[[ 0.1323,  0.3435,  0.1365,  ...,  0.2833, -0.2909,  0.3727],\n",
      "         [ 0.4390, -0.0905,  0.2218,  ..., -0.3375,  0.3586,  0.3006],\n",
      "         [ 1.1752,  0.4498, -0.6960,  ...,  0.1995,  0.8212, -0.1582]]],\n",
      "       grad_fn=<NativeLayerNormBackward>)\n"
     ]
    }
   ],
   "source": [
    "sentences = ['[MASK]']\n",
    "input_ids = tokenizer(sentences, padding=True, return_tensors='pt')\n",
    "print(input_ids)\n",
    "lm_outputs = lm_model(**input_ids)\n",
    "transformer_hidden_states = lm_outputs[-1]\n",
    "print(transformer_hidden_states[-1].shape)\n",
    "print(transformer_hidden_states[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "overall-recycling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101, 19082,   103,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n",
      "torch.Size([1, 4, 768])\n",
      "tensor([[[ 0.4933,  0.3707,  0.2677,  ..., -0.2835,  0.5303, -0.0569],\n",
      "         [ 0.5455, -0.4865,  0.7949,  ..., -0.5479,  0.1502,  0.0675],\n",
      "         [ 0.4508,  0.3423,  0.4286,  ..., -0.5160,  0.4282, -0.0413],\n",
      "         [ 0.9906,  0.0174, -0.2410,  ..., -0.1005,  1.2535, -0.3444]]],\n",
      "       grad_fn=<NativeLayerNormBackward>)\n"
     ]
    }
   ],
   "source": [
    "sentences = ['hello [MASK]']\n",
    "input_ids = tokenizer(sentences, padding=True, return_tensors='pt')\n",
    "print(input_ids)\n",
    "lm_outputs = lm_model(**input_ids)\n",
    "transformer_hidden_states = lm_outputs[-1]\n",
    "print(transformer_hidden_states[-1].shape)\n",
    "print(transformer_hidden_states[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "waiting-litigation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[18435,   995, 23748]]), 'attention_mask': tensor([[1, 1, 1]])}\n",
      "torch.Size([1, 3, 1600])\n",
      "tensor([[[ 1.0588,  0.1301,  0.2188,  ..., -4.4375,  0.7642,  0.2327],\n",
      "         [-0.2217,  0.3650,  0.6309,  ..., -1.3968,  0.1588,  0.1691],\n",
      "         [ 0.0775,  0.1162,  0.7933,  ..., -1.5378, -0.0938,  0.9396]]],\n",
      "       grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "sentences = ['hello world hello']\n",
    "input_ids = tokenizer(sentences, padding=True, return_tensors='pt')\n",
    "print(input_ids)\n",
    "lm_outputs = lm_model(**input_ids)\n",
    "transformer_hidden_states = lm_outputs[-1]\n",
    "print(transformer_hidden_states[-1].shape)\n",
    "print(transformer_hidden_states[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "acting-inspection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[18435,   995,   612,   345]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n",
      "torch.Size([1, 4, 1600])\n",
      "tensor([[[ 1.0588,  0.1301,  0.2188,  ..., -4.4375,  0.7642,  0.2327],\n",
      "         [-0.2217,  0.3650,  0.6309,  ..., -1.3968,  0.1588,  0.1691],\n",
      "         [ 0.4922,  0.5799,  0.3748,  ..., -1.2212,  0.4659, -0.0181],\n",
      "         [ 1.1680, -0.2796, -0.2864,  ..., -1.5432,  0.7543, -0.8277]]],\n",
      "       grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "sentences = ['Hello world there you']\n",
    "input_ids = tokenizer(sentences, padding=True, return_tensors='pt')\n",
    "print(input_ids)\n",
    "lm_outputs = lm_model(**input_ids)\n",
    "transformer_hidden_states = lm_outputs[-1]\n",
    "print(transformer_hidden_states[-1].shape)\n",
    "print(transformer_hidden_states[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "normal-capability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[18435,   995,   612,   345,   389]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n",
      "torch.Size([1, 5, 1600])\n",
      "tensor([[[ 1.0588,  0.1301,  0.2188,  ..., -4.4375,  0.7642,  0.2327],\n",
      "         [-0.2217,  0.3650,  0.6309,  ..., -1.3968,  0.1588,  0.1691],\n",
      "         [ 0.4922,  0.5799,  0.3748,  ..., -1.2212,  0.4659, -0.0181],\n",
      "         [ 1.1680, -0.2796, -0.2864,  ..., -1.5432,  0.7543, -0.8277],\n",
      "         [ 0.2596,  0.3958,  0.6677,  ..., -1.7543,  0.9028,  0.6392]]],\n",
      "       grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "sentences = ['Hello world there you are']\n",
    "input_ids = tokenizer(sentences, padding=True, return_tensors='pt')\n",
    "print(input_ids)\n",
    "lm_outputs = lm_model(**input_ids)\n",
    "transformer_hidden_states = lm_outputs[-1]\n",
    "print(transformer_hidden_states[-1].shape)\n",
    "print(transformer_hidden_states[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "unlike-encoding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 995,  612,  345,  389, 1029]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n",
      "torch.Size([1, 5, 1600])\n",
      "tensor([[[-0.4205,  0.6139,  1.0727,  ..., -4.7529,  1.2246, -0.4381],\n",
      "         [-0.1484, -0.2284,  0.3114,  ..., -1.4831,  0.4668, -0.4868],\n",
      "         [-0.0156, -0.6524,  0.0106,  ..., -1.2291,  0.0511, -0.2832],\n",
      "         [-1.7830,  0.0059,  0.2923,  ..., -1.4245, -0.0341,  0.6478],\n",
      "         [-0.9276, -0.2381, -0.5200,  ..., -0.9828,  0.4576,  0.4824]]],\n",
      "       grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "sentences = ['world there you are high']\n",
    "input_ids = tokenizer(sentences, padding=True, return_tensors='pt')\n",
    "print(input_ids)\n",
    "lm_outputs = lm_model(**input_ids)\n",
    "transformer_hidden_states = lm_outputs[-1]\n",
    "print(transformer_hidden_states[-1].shape)\n",
    "print(transformer_hidden_states[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "stock-fetish",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "dl = data.DataLoader(input_ids, batch_size=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('247-main')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "e13a80e14d77696cc32872df194f7ce7048410ca2fad75a02a9901565086d535"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
